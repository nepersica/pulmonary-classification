{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(f\"/home/ncp/workspace/seung-ah/train.txt\",'r').read().splitlines()\n",
    "val = open(f\"/home/ncp/workspace/seung-ah/val.txt\",'r').read().splitlines()\n",
    "test = open(f\"/home/ncp/workspace/seung-ah/test.txt\",'r').read().splitlines()\n",
    "\n",
    "dataset = train+val+test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save hdf5 files by single cpu processor\n",
    "'''\n",
    "class hdf5_Converter:\n",
    "    def __init__(self, dataset, save_dir, mode, root_dir='/home/ncp/workspace/data/train'):\n",
    "        self.save_dir = save_dir\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.mode = mode\n",
    "        self.data_list = []\n",
    "\n",
    "    def start_parsing(self):\n",
    "        self._get_infarct_data()\n",
    "        print(len(self.data_list))\n",
    "        print(f'Finish Getting DICOM datas...\\n')\n",
    "        self._save_image_to_hdf5()\n",
    "\n",
    "    def _get_infarct_data(self):\n",
    "        # f = open('/utils/dataset/KNUH/CheckingDescription.txt', 'w')\n",
    "        for data in tqdm(self.dataset):\n",
    "            # json 파일 정보 읽어들이기\n",
    "            with open(data, 'r', encoding='UTF8') as f:\n",
    "                content = json.load(f)\n",
    "            file_name = content['identifier']\n",
    "            dicom_path = os.path.join(self.root_dir, content['mask_image']['org_dicom_file'][4:])\n",
    "\n",
    "            label = int(content['patient']['diagnosis'])\n",
    "            if label == 9:\n",
    "                label = 7\n",
    "\n",
    "            f.close()\n",
    "            self.data_list.append([dicom_path, label, file_name])\n",
    "\n",
    "\n",
    "    def _save_image_to_hdf5(self):\n",
    "        save_path = os.path.join(self.save_dir, self.mode)\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        for data in tqdm(self.data_list):\n",
    "            dicom_path = data[0]\n",
    "            class_id = data[1]\n",
    "            file_name = data[2]\n",
    "\n",
    "            output_path = os.path.join(save_path, f'{file_name}.hdf5')\n",
    "            \n",
    "            with h5py.File(output_path, 'w') as hf:  # open a hdf5 file\n",
    "                dcm = pydicom.dcmread(dicom_path)\n",
    "                image = dcm.pixel_array\n",
    "                \n",
    "                image_size=256\n",
    "                # resize image\n",
    "                dim = (image_size, image_size)\n",
    "                image = cv.resize(image, dim, interpolation = cv.INTER_AREA)\n",
    "                \n",
    "                if isinstance(image, (np.ndarray, np.generic)):\n",
    "                    if len(image.shape) == 3:\n",
    "                        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                HEIGHT, WIDTH = image.shape\n",
    "\n",
    "                _image = hf.create_dataset(\n",
    "                    name='input',\n",
    "                    data=image,\n",
    "                    shape=(HEIGHT, WIDTH),\n",
    "                    maxshape=(HEIGHT, WIDTH),\n",
    "                    compression=\"gzip\",\n",
    "                    compression_opts=9)\n",
    "                \n",
    "                _label = hf.create_dataset(name='class_id', data=[class_id])\n",
    "\n",
    "                hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_hdf5 = hdf5_Converter(train,  \"/home/ncp/workspace/seung-ah/hdf5\", 'train')\n",
    "convert_hdf5.start_parsing()\n",
    "convert_hdf5 = hdf5_Converter(train,  \"/home/ncp/workspace/seung-ah/hdf5\", 'val')\n",
    "convert_hdf5.start_parsing()\n",
    "convert_hdf5 = hdf5_Converter(train,  \"/home/ncp/workspace/seung-ah/hdf5\", 'test')\n",
    "convert_hdf5.start_parsing()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
